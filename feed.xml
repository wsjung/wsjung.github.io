<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jungwooseok.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jungwooseok.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-28T21:12:36+00:00</updated><id>https://jungwooseok.com/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Woo Seok Jung </subtitle><entry><title type="html">METANet: Reconstructing Tissue-specific TF Network Maps</title><link href="https://jungwooseok.com/blog/2026/metanet-biorxiv/" rel="alternate" type="text/html" title="METANet: Reconstructing Tissue-specific TF Network Maps"/><published>2026-01-27T11:37:00+00:00</published><updated>2026-01-27T11:37:00+00:00</updated><id>https://jungwooseok.com/blog/2026/metanet-biorxiv</id><content type="html" xml:base="https://jungwooseok.com/blog/2026/metanet-biorxiv/"><![CDATA[<h2 id="unraveling-tissue-specific-gene-regulation-with-metanet">Unraveling Tissue-Specific Gene Regulation with METANet</h2> <p>Transcription factors (TFs) act as the “switches” of the genome, controlling which genes are turned on or off in specific biological contexts. Mapping these regulatory networks is essential for understanding the genetic mechanisms underlying complex human traits and diseases. However, accurately reconstructing these networks remains a significant computational challenge.</p> <p>Current methods generally fall into two categories, each with limitations. Motif-based approaches scan the genome for specific DNA sequences where TFs could bind, but they often lack evidence that the binding actually changes gene expression. Conversely, expression-based methods look for correlations between TF and gene activity, but they struggle to distinguish direct physical regulation from indirect effects or co-regulation.</p> <h2 id="introducing-metanet">Introducing METANet</h2> <p>To bridge this gap, my colleagues and I developed METANet, a supervised ensemble learning framework designed to reconstruct direct and functional tissue-specific TF networks. METANet integrates diverse genomic data types to predict the probability that a TF binds to a target gene.</p> <p>The framework combines five distinct sets of regulatory features to model gene regulation:</p> <ul> <li><strong>Motif-based features</strong>: Leveraging tissue-specific cis-regulatory element activity to determine binding potential.</li> <li><strong>Expression-based features</strong>: Utilizing both linear (LASSO) and non-linear (BART) regression models to capture complex regulatory relationships between TFs and target genes.</li> </ul> <p>By training on high-confidence ChIP-seq binding data, METANet learns to prioritize targets that are supported by both physical binding evidence and functional expression patterns.</p> <h2 id="key-findings">Key Findings</h2> <p>We applied this framework to generate comprehensive network maps for 36 human tissues, covering approximately 237 TFs and over 12,150 genes. Our evaluations demonstrate that METANet significantly outperforms established benchmarks (such as PANDA and Marbach networks) in identifying direct, functional targets validated by biological data. Furthermore, we showed that these networks can successfully prioritize disease-associated genes, such as identifying <em>SREBF2</em> as a key regulator in cardiovascular risk traits.</p> <h2 id="read-the-full-paper">Read the Full Paper</h2> <p>This research is currently available as a preprint on bioRxiv and is under review for publication. You can read the full manuscript here: <a href="https://doi.org/10.1101/2025.10.10.681634">METANet: A supervised ensemble learning framework for reconstructing direct and functional tissue-specific transcription factor networks</a>.</p> <h2 id="exploring-the-networks">Exploring the Networks</h2> <p>To make these complex regulatory maps accessible, I have created a simple interactive dashboard using Tableau using our networks that are available on <a href="https://doi.org/10.5281/zenodo.17309371">Zenodo</a>. The visualization below allows you to explore the TF-target enrichments and visualize the regulatory architecture across the different human tissues we analyzed.</p> <hr/> <div id="vizContainer" style="width:100%; height:850px; overflow:hidden;"> <div class="tableauPlaceholder" id="viz1769585260706" style="position: relative"><noscript><a href="#"><img alt="Top 20 edges in METANet by tissue " src="https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ME&#47;METANettop20edgesbytissue&#47;Top20edgesinMETANetbytissue&#47;1_rss.png" style="border: none"/></a></noscript><object class="tableauViz" style="display:none;"><param name="host_url" value="https%3A%2F%2Fpublic.tableau.com%2F"/> <param name="embed_code_version" value="3"/> <param name="site_root" value=""/><param name="name" value="METANettop20edgesbytissue&#47;Top20edgesinMETANetbytissue"/><param name="tabs" value="no"/><param name="toolbar" value="no"/><param name="static_image" value="https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ME&#47;METANettop20edgesbytissue&#47;Top20edgesinMETANetbytissue&#47;1.png"/> <param name="animate_transition" value="yes"/><param name="display_static_image" value="yes"/><param name="display_spinner" value="yes"/><param name="display_overlay" value="yes"/><param name="display_count" value="yes"/><param name="language" value="en-US"/><param name="filter" value="publish=yes"/></object></div> <script type="text/javascript">                    var divElement =
document.getElementById('viz1769585260706');                    var vizElement =
divElement.getElementsByTagName('object')[0];                    if (
divElement.offsetWidth > 800 ) {
vizElement.style.minWidth='1020px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';}
else if ( divElement.offsetWidth > 500 ) {
vizElement.style.minWidth='1020px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';}
else { vizElement.style.width='100%';vizElement.style.height='727px';}
var scriptElement = document.createElement('script');
scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';
vizElement.parentNode.insertBefore(scriptElement, vizElement);
</script> </div> <hr/>]]></content><author><name></name></author><category term="research"/><category term="visualization"/><category term="GRN"/><category term="ML"/><summary type="html"><![CDATA[Ensemble learning for inferring GRNs for 36 human tissues]]></summary></entry><entry><title type="html">First Publication!</title><link href="https://jungwooseok.com/blog/2022/first-publication/" rel="alternate" type="text/html" title="First Publication!"/><published>2022-06-10T08:34:00+00:00</published><updated>2022-06-10T08:34:00+00:00</updated><id>https://jungwooseok.com/blog/2022/first-publication</id><content type="html" xml:base="https://jungwooseok.com/blog/2022/first-publication/"><![CDATA[<p>It has been a long time since my last blog post, but the news is that I published my first paper! It’s been a year and a half since I joined the Brent Lab and I have seen my rotation project project <em>Predicting which genes will respond to transcription factor perturbations</em> come to fruition.</p> <p>Briefly, the paper aims to predict which genes will respond to the perturbation of a particular transcription factor (TF) in yeast and human cells. We use XGBoost gradient-boosted trees integrating pre-perturbation gene expression level and variation, histone modifications, chromatin accessibility, and ChIP-seq TF-binding location data to predict whether a gene is “responsive” to a TF perturbation. In human cells, a gene is considered responsive based on whether its DESeq2 result are significant at P&lt;0.05 and its absolute log2 fold-change is greater than 0.5. One of the core analyses of the paper is the quantification of feature influences. That is, determining which of the input features is the most predictive. The paper finds that in humans, pre-perturbation gene expression level and variation were the most influential whereas TF binding location was least informative. This comes at a surprise as ChIP-seq data is often regarded as one of the most useful sources of evidence for potential TF regulation of a gene.</p> <p>The lack of TF binding location predictive power in human cells is interesting, particularly because it is one of the most useful features in the yeast model. One way we could attribute this result is the size and complexity of the human genome. This may be evidence that in humans, TF regulation may be more mediated by enhancers that are further from the gene body.</p> <p>Check out the paper published on G3 at the following link: https://doi.org/10.1093/g3journal/jkac144.</p>]]></content><author><name></name></author><category term="research"/><category term="ML"/><summary type="html"><![CDATA[It has been a long time since my last blog post, but the news is that I published my first paper! It’s been a year and a half since I joined the Brent Lab and I have seen my rotation project project Predicting which genes will respond to transcription factor perturbations come to fruition.]]></summary></entry><entry><title type="html">Appointment to the GATP</title><link href="https://jungwooseok.com/blog/2022/appointment-to-the-gatp/" rel="alternate" type="text/html" title="Appointment to the GATP"/><published>2022-02-03T21:20:00+00:00</published><updated>2022-02-03T21:20:00+00:00</updated><id>https://jungwooseok.com/blog/2022/appointment-to-the-gatp</id><content type="html" xml:base="https://jungwooseok.com/blog/2022/appointment-to-the-gatp/"><![CDATA[<p>Today, I was appointed to the WashU Genome Analysis Training Program (GATP). The program name may be unfamiliar as it’s specific to the institution, but the this program is funded by the NHGRI grant T32 which is officially titled “Institutional training grant in genomic science”. Now, let’s just note what this appointment actually entails.</p> <p>As this grant is for training students for genome analysis, it expects appointees take courses that fulfill requirements in computer programming, probability and statistics, cell and molecular biology, and genome technologies and computational biology. The first two I should already have satisfied with courses taken in my undergraduate university. However, because I have not taken an actual college-level biology course, I will need to take courses here at WashU to fulfill the latter two requirements.</p> <p>What’s also interesting is that this grant sponsors attendance of the annual NHGRI Research Training and Career Development Annual Meeting which happens each spring. The next one is in a few weeks at Duke University. They expect that trainees give poster presentations and considering the deadline to submit a PDF poster is in less than a week, it’s a little daunting.</p> <p>I think this appointment is going to be a good opportunity for me. It’s been about 9 months since I’ve joined the Brent Lab and I have noticed my limited knowledge of biology and genomics holding me back occasionally. The required courses and opportunities to communicate my project ideas and results with experts in the area will be useful in setting my foot in the field.</p>]]></content><author><name></name></author><category term="misc"/><category term="misc"/><summary type="html"><![CDATA[Today, I was appointed to the WashU Genome Analysis Training Program (GATP). The program name may be unfamiliar as it’s specific to the institution, but the this program is funded by the NHGRI grant T32 which is officially titled “Institutional training grant in genomic science”. Now, let’s just note what this appointment actually entails.]]></summary></entry><entry><title type="html">Lab Affiliation</title><link href="https://jungwooseok.com/blog/2021/lab-affiliation/" rel="alternate" type="text/html" title="Lab Affiliation"/><published>2021-03-08T17:49:00+00:00</published><updated>2021-03-08T17:49:00+00:00</updated><id>https://jungwooseok.com/blog/2021/lab-affiliation</id><content type="html" xml:base="https://jungwooseok.com/blog/2021/lab-affiliation/"><![CDATA[<p>As my third lab rotation I joined Dr. Michael Brent’s Computational Genomics lab. Genomics is a rather foreign subject for me as it’s been over 8 years since I’ve touched anything Biology related. Actually, I have done a project in my last years as an undergraduate under Dr. Adam Smith where we tried to simulate mouse and bird vocalizations via GANs. Yet, genomics still felt very much out of reach when I started my rotation.</p> <p>With a lot of Wikipedia articles, youtube videos, and paper reading I have garnered few bits of Genomics knowledge over the rotation period. Specifically, I have learned a lot about the structure and function of genes and gene regulation. During the rotation, I was tasked with getting up to speed with the codebase of an existing pre-print: <a href="Predicting which genes will respond to perturbations of a TF: TF-independent properties of genes are major determinants of their responsiveness.">https://www.biorxiv.org/content/10.1101/2020.12.15.422864v1</a> I was able to find some alternative perspectives on existing results under the direction of Dr. Brent.</p> <p>I found my experiences during this rotation exciting and decided to join the lab officially! I am happy with how supportive the lab members are especially considering I have little to no background Genomics knowledge. Fun fact is that many graduate researchers in the lab also started out on a similar note, expanding their knowledge base over the years.</p> <p>For the immediate future, I will continue working on the pre-print that I have been working on during the rotation, applying the technique to a completely different dataset to see whether we arrive at similar results.</p> <p>I’m really happy with the results of this rotation and look forward to what projects I will undertake as a member of the Brent Lab.</p>]]></content><author><name></name></author><category term="misc"/><category term="misc"/><summary type="html"><![CDATA[As my third lab rotation I joined Dr. Michael Brent’s Computational Genomics lab. Genomics is a rather foreign subject for me as it’s been over 8 years since I’ve touched anything Biology related. Actually, I have done a project in my last years as an undergraduate under Dr. Adam Smith where we tried to simulate mouse and bird vocalizations via GANs. Yet, genomics still felt very much out of reach when I started my rotation.]]></summary></entry><entry><title type="html">Rotation 2 Completion</title><link href="https://jungwooseok.com/blog/2020/rotation2-completion/" rel="alternate" type="text/html" title="Rotation 2 Completion"/><published>2020-12-16T22:09:00+00:00</published><updated>2020-12-16T22:09:00+00:00</updated><id>https://jungwooseok.com/blog/2020/rotation2-completion</id><content type="html" xml:base="https://jungwooseok.com/blog/2020/rotation2-completion/"><![CDATA[<p>This week I have finished my second research rotation with Professor Neal Patwari’s lab. This second rotation happened to be over a much busier time period of the semester as it overlapped with a couple midterms at its start and few finals in its completion. Nonetheless, here is a casual report on what I did over this rotation and my overall thoughts.</p> <p>Before the start of the rotation, as I also did with my first as per rotation requirements, I drafted a rotation proposal with Neal. This proposal covers a brief overview of the goals and expected timeline of the lab rotation. As all things are, the proposal does not define or limit exactly what I do during the time period. Anyway, the initial goal for this rotation was to develop a set of unsupervised machine learning models that would predict line-of-sight (LOS) from multipath and non-line-of-sight (NLOS) signals using a dataset gathered over the POWDER platform.</p> <p>To clarify, <a href="POWDER">https://powderwireless.net/</a> (Platform for Open Wireless Data-driven Experimental Research) is a cluster for state-of-the-art networking research. It consists of a network of cutting-edge hardware physically located at the University of Utah campus. What we are interested in are the software-defined radios (SDRs) attached to various buildings around campus that are configurable to either transmit or receive signals at specified frequencies.</p> <p>LOS signals describe the transmission of a radio signal from the transmitter to the receiver in a direct path. This means there is no object interfering with the signal propagating in a straight line drawn from the transmitter to the receiver. Multipath signals are characterized by a means of arriving to the receiver via reflection(s) off of surfaces. These multipath signals can also be seen in cases where the transmitter has an unobstructed path to the receiver. NLOS signals are ones that are received when the direct path between the transmitter and receiver is obstructed. However, this does not limit NLOS signals to only multipath signals. In fact, radio waves can pass through walls and reach the receiver in the shortest path but the received signal will be much weaker or transformed.</p> <p>A goal for the lab’s POWDER project is to localize a mobile transmitter via a network of receivers. POWDER is currently testing and planning on releasing a node on the platform which is a radio on a campus bus. Before that, the lab aimed to test localization accuracy using fixed SDRs on the buildings.</p> <p>Unfortunately, data was not collected when I started the rotation and I was instead asked to help collect this data. The existing data collection pipeline consisted of a set of Python scripts that made use of the multiprocessing library. However, there were limitations to this approach as what was needed was real-time concurrent remote-execution of transmitter and receiver scripts. Thus I settled on the Python Fabric library which allows the concurrent execution of scripts over a set of 10+ SSH connections in a parallel manner. Towards the end of the rotation, over a short POWDER reservation of 10 nodes, the data collection pipeline successfully amassed 6GB of test data.</p> <p>This feels like a short post as did this rotation. I do feel a little bit disappointed in that I did not in fact get to do much or any ML modeling but I sure enjoyed building a solid data collection pipeline. The lab happened to also be on the smaller-scale side as it is a relatively recently established lab in the department. As of now, I am hoping to try one more rotation in the Spring before I decide what is the best course of action for me.</p>]]></content><author><name></name></author><category term="misc"/><category term="misc"/><summary type="html"><![CDATA[This week I have finished my second research rotation with Professor Neal Patwari’s lab. This second rotation happened to be over a much busier time period of the semester as it overlapped with a couple midterms at its start and few finals in its completion. Nonetheless, here is a casual report on what I did over this rotation and my overall thoughts.]]></summary></entry><entry><title type="html">Rotation 1 Completion</title><link href="https://jungwooseok.com/blog/2020/rotation1-completion/" rel="alternate" type="text/html" title="Rotation 1 Completion"/><published>2020-11-15T00:00:00+00:00</published><updated>2020-11-15T00:00:00+00:00</updated><id>https://jungwooseok.com/blog/2020/rotation1-completion</id><content type="html" xml:base="https://jungwooseok.com/blog/2020/rotation1-completion/"><![CDATA[<p>It’s been almost a month since my last blog post and my excuse is that I’ve been quite busy! So as today marks the end of my first PhD research rotation with Dr. Netanel Raviv, this blog post will consists of my work during the rotation and some thoughts regarding it.</p> <p>As I explained in my last blog post, my rotation with Dr. Raviv dealt with exploring whether there exists a better means of coding a perceptron with improved minimum distance and relative distance than a parity code. This can be formulated equivalently by proving the existence of a \((d,\theta)\)-polarizing code by picking a generator matrix at random and analyzing the resulting code is indeed \((d,\theta)\)-polarizing which is defined as follows:</p> \[\textrm{if} \ w_H (\mathbf{x}) &gt; \frac{k-\theta}{2} \quad \textrm{then} \ w_H(E(\mathbf{x})) &gt; \frac{n - \mu + d}{2} (heavy-to-heavy)\] \[\textrm{if} \ w_H (\mathbf{x}) &lt; \frac{k-\theta}{2} \quad \textrm{then} \ w_H(E(\mathbf{x})) &lt; \frac{n-\mu-d}{2} (light-to-light)\] <p>where \(k\) is the size of the information word, \(n\) is the size of the coded word, and \(d\) determines how strictly heavy or light we want the resulting coded word to be.</p> <p>Our method of approaching this proof consisted of picking the generator matrix at random, computing the probability that one \(t\)-weight information word is mapped correctly, and then understand the probability that all information words in \(F_2^k\) are mapped correctly using the Union Bound.</p> <p>The generator matrix is a \(k \times n\) matrix where each entry is a Bernoulli random variable with probability \(p\) that \(g_{r,c} = 1\) and probability \(q = 1-p\) that \(g_{r,c} = 0\). An information word \(\mathbf{u}\) is a \(k \times 1\) vector where each entry is also binary \(u_i \in \{0, 1\}\). Each information word has a weight \(t = w_H (\mathbf{u})\) which is the Hamming weight of the word.</p> <p>My initial attempt at this proof was an utter failure as I failed to take into consideration that we are working in \(F_2^k\) which is a Galois Field of dimension 2. This meant that under this assumption of a binary field, arithmetic uses binary operations: addition of elements is actually done using the exlcusive or (XOR) and multiplication using AND. Binary multiplication in \(F_2 = \{0, 1\}\) remains the same but the main problem was with binary addition.</p> <p>Fixing this stupid error on my part, I found the probability that the \(i^{th}\) entry evaluates to 1. Since the \(i^{th}\) entry of the coded word is</p> \[u_1 g_{1,i} \oplus u_2 g_{2,i} \oplus \cdots \oplus u_k g_{k,i} = \overset{t}{\underset{\ell=1}{\oplus}} g_{j_\ell,i},\] <p>where \(u_{j_\ell} = 1\) for \(\ell = 1, \ldots, t\). Thus the probability of a 1 at the \(i^{th}\) entry is</p> \[P[\overset{t}{\underset{\ell=1}{\oplus}} g_{j_\ell,i} = 1] = \sum_{\underset{n \textrm{ is odd }}{n=1}}^{t} \textrm{Binom}(t,n).\] <p>We will define this as \(c(t,p)\) such that the coded word \(\mathbf{u}G\) is a \(1 \times n\) matrix of \(c(t,p)\)’s. Now, to find the probability of the coded word being strictly light is when the Hamming weight of the coded word is less than \(\frac{n-d}{2}\) which can be written as</p> \[P[\textrm{light } \mathbf{u}G] = \sum_{i=1}^{\lfloor \frac{n}{2} + d - 1 \rfloor} \textrm{Binom}(n,i)\] <p>where \(\textrm{Binom}(n,i)\) is a success with probability \(c(t,p)\). The probability of the coded word being strictly heavy is similar with its appropriate bounds on the sum.</p> <p>The “good” event for one information word \(\mathbf{u}\) is that \(\mathbf{u}\) is mapped correctly. Its complement (the “bad” event) is one where \(\mathbf{u}\) is mapped incorrectly. We will denote these events by \(e(\mathbf{u}) \textrm{ and } \neg e(\mathbf{u})\), respectively. The “good” event consists of all information words in \(F_2^k\) being mapped correctly:</p> \[P[E] = P[\underset{\mathbf{u} \in F_2^k}{\bigcap} e(\mathbf{u})]\] <p>where we want to show \(\)P[E] &gt; 0\(\) since a non-zero probability implies that such an encoding is possible with a randomly chosen generator matrix (code). However, since it is difficult to compute the probability of the intersection of these “good” events, we will instead compute its complement</p> \[P[E^c] = P[\underset{\mathbf{u} \in F_2^k}{\bigcup} \neg e(\mathbf{u})],\] <p>and show that \(P[E^c] &lt; 1\). Here, applying the Union Bound, we can alternatively show that</p> \[P[\underset{\mathbf{u} \in F_2^k}{\bigcup} \neg e(\mathbf{u})] \leq \sum_{\mathbf{u} \in F_2^k} P[\neg e(\mathbf{u})] &lt; 1.\] <p>Expanding out \(P[E^c]\), we get</p> \[P[E^c] = \sum_{t=0}^{\lfloor \frac{k}{2}} {k \choose t} P[\textrm{heavy } \mathbf{u}G] + \sum_{t = \lceil \frac{k}{2} \rceil}^{k} {k \choose t} P[\textrm{light } \mathbf{u}G].\] <p>I wrote a MATLAB script to test different values for the parameters \(k,n,d,p\) but the results showed that we in fact do not get any probabilities below 1 for \(k &gt; 1\). This is not a good sign and adjusting the threshold for light/heavy categorization did not change this result either. Thus, my conclusion is that either the Union Bound is too weak or in fact a \((d,\theta)\)-polarizing code does not exist for \(k &gt; 1\).</p> <p>I would have liked to continue investigating this disappointing result but at this time, my rotation with Dr. Raviv has come to an end and I must prepare for my next rotation with Dr. Neal Patwari in the ESE department. Regarding my thoughts on this rotation, it was my first time dealing with such theory-heavy research but I think I came to like this sort of work. It’s definitely a change from more practical and/or application-wise research and it relies heavily on math and probability theory (especially moreso as this is ML research). At this point, I’m not entirely sure if I would enjoy the idea of pursuing a theoretical research path for my PhD as it sort of limits the opportunities in industry after graduation. And at the moment, personally, I swing a bit more towards working in industry than staying in academia post-grad. Well, my next rotation with Dr. Patwari will be on the completely opposite spectrum where I will likely be developing a ML application so at the end of my second rotation I will have a better sense of the kind of research I enjoy. :)</p>]]></content><author><name></name></author><category term="misc"/><category term="misc"/><summary type="html"><![CDATA[It’s been almost a month since my last blog post and my excuse is that I’ve been quite busy! So as today marks the end of my first PhD research rotation with Dr. Netanel Raviv, this blog post will consists of my work during the rotation and some thoughts regarding it.]]></summary></entry><entry><title type="html">Introduction to Coding Theory</title><link href="https://jungwooseok.com/blog/2020/intro-to-coding-theory/" rel="alternate" type="text/html" title="Introduction to Coding Theory"/><published>2020-10-18T00:00:00+00:00</published><updated>2020-10-18T00:00:00+00:00</updated><id>https://jungwooseok.com/blog/2020/intro-to-coding-theory</id><content type="html" xml:base="https://jungwooseok.com/blog/2020/intro-to-coding-theory/"><![CDATA[<p>It’s been a while since the genesis of this blog and its first post. Quite a shame I couldn’t keep up with the weekly blog commitment since the first week. At this point, I have just started my first research rotation with Professor Netanel Raviv to explore whether there exist better means of coding a perceptron with improved minimum and relative distance metrics than a parity code. To help my understanding of the topic which is rooted in Coding Theory, I read through several chapters of Ron Roth’s textbook “Introduction to Coding Theory” and in this blog I will note what I found most interesting and relevant to my research rotation.</p> <p>The goal is to better grasp linear codes. But before I cover linear codes, I will first cover some basic concepts of channel coding. Figure 1.1 is a graphic of the overview of communications system that transmits a message from a Source to a Destination. The Source Encoder serves two roles: (1) translating the output of the source to the input to the channel and (2) compressing the source output, if needed. The compression determines whether a channel is <em>lossy</em> or <em>lossless</em>. What’s to note is that due to physical limitations of communications systems in real life, the message received may (usually) differ from the message transmitted due to noise.</p> <p><img src="https://jungwooseokcom.wordpress.com/wp-content/uploads/2020/10/fig1.png" alt="fig1"/></p> <p>Channels will be modeled as a discrete probabilistic channel where the channel encoder takes in an information word (message) and outputs a <em>codeword</em> which the channel outputs as a received word, then decoded by the channel decoder as a decoded <em>codeword</em> and decoded information. The goal of the system is to make sure that the decoded information word matches the information word and the decoded <em>codeword</em> matches the <em>codeword</em>.</p> <p>We assume that it is more likely to less errors than more errors and errors are introduced to each element of a message independently and with a fixed probability p &lt; 1/2. The <em>Hamming distance</em> is the metric of choice to measure to “number of errors”. The Hamming distance is defined as the number of different elements between two words. The distance \(d\) of a \((n,M)\) code is the minimum Hamming distance between any two of its codewords. The Hamming weight of a codeword is the number of non-zero entries. This is equivalent as the Hamming distance of a codeword with the zero codeword.</p> <p>Appropriately, the code is called an \((n,M,d)\) code where the n, M parameters are the code length and code size, respectively. By <em>nearest neighbor decoding</em>, received words are decoded to its closest codeword (codeword of the minimum Hamming distance with the received word). <strong>Thus, the larger the minimum distance of a code, the better.</strong></p> <p>Now on to the topic of linear codes. Linear codes are defined as a code \(C\) where a linear combination of any two codewords is also a codeword in \(C\). In short, a linear code is a vector subspace. The dual code of \(C\) is the orthogonal complement of \(C\) in its alphabet subspace. A linear code is written as a \([n,k,d]\) code where \(k\) is the dimension of the code. A linear code has a <strong>nonunique</strong> \(k x n\) <em>generator matrix</em> \(G\) whose rows form a basis of the code. The parity check matrix of \(H\) for \(C\) is a generator matrix for the dual code of \(C\). A systematic generator matrix is a \((I | A)\) matrix and these exist when the first \(k\) columns of any generator matrix of \(C\) is linearly independent. Systematic generator matrices are what simplify encoding and decoding of linear codes.</p> <p>The source encoder maps an information word \(u\) to a codeword \(c\) by matrix multiplication with the code’s generator matrix \(c = uG\). For a systematic generator matrix, this is equivalent to \(c = (u | uA)\) where \(A\) is a \(k x (n-k)\) matrix.</p> <p>Decoding methods rely on a standard array for a code which is a 2-D array in which the first row consists of the codewords of C starting with the zero codeword. Subsequent rows (cosets) start with a word of smallest Hamming weight e followed by words \(e + c\). Decoding takes the following steps: (1) Find the row that contains received word \(y\). (2) The codeword c is the first entry in the column containing \(y\). Step 2 is effectively computing \(c = y - e\).</p> <p>One decoding method is <em>syndrome decoding</em>. A <em>syndrome</em> \(s\) of a word \(y\) is defined as \(s = Hy^{T}\). It is important to note that since a code \(C\) is a kernel of \(H\), the syndrome of any codeword in \(C\) is the zero vector. Syndrome decoding takes two steps: (1) Finding the syndrome \(s\) of the received word \(y\). (2) Finding the minimum-weight word e such that \(s = He^{T}\). This is computationally intractable and requires a look-up table unless the <em>redundancy</em> \(n-k\) is small.</p> <p>One of the advantages of working with linear codes is that everything boils down to linear algebra. A code can be describe using its basis, the code’s distance is its weight, and these together make the mapping of a message to code (and vice-versa) simple. This is the main reason why linear codes are the most prevalent codes used today.</p>]]></content><author><name></name></author><category term="misc"/><category term="misc"/><summary type="html"><![CDATA[It’s been a while since the genesis of this blog and its first post. Quite a shame I couldn’t keep up with the weekly blog commitment since the first week. At this point, I have just started my first research rotation with Professor Netanel Raviv to explore whether there exist better means of coding a perceptron with improved minimum and relative distance metrics than a parity code. To help my understanding of the topic which is rooted in Coding Theory, I read through several chapters of Ron Roth’s textbook “Introduction to Coding Theory” and in this blog I will note what I found most interesting and relevant to my research rotation.]]></summary></entry><entry><title type="html">Starting a weekly blog</title><link href="https://jungwooseok.com/blog/2020/starting-a-blog/" rel="alternate" type="text/html" title="Starting a weekly blog"/><published>2020-09-30T00:00:00+00:00</published><updated>2020-09-30T00:00:00+00:00</updated><id>https://jungwooseok.com/blog/2020/starting-a-blog</id><content type="html" xml:base="https://jungwooseok.com/blog/2020/starting-a-blog/"><![CDATA[<p>Today marks the third week since the start of my PhD program at Washington University at St. Louis. Inspired by fellow PhD student Saumik, I’ve decided to commit to writing a weekly blog to track the progress of my maturation as a PhD student. You can check out Saumik’s website here.</p> <p>New to the blogging scene, the first few posts will be littered with rough edges and the website in general will be rather bland. I have lately felt a need to write as it has been years since the days of writing-intensive liberal arts courses resulting in a noticeable degradation in my writing and flow of thinking in general. Hopefully this commitment to weekly blogging will help develop a writing habit that will keep me thinking and bring some structure to this disorderly COVID-19 schedule.</p> <p>Besides the goal of putting my life together again, writing is a crucial skill to hone as a PhD student. During the next several years, I will need to write numerous papers on prestigious journals and conferences whose primary audience is other scientists in the field. This will require a very specific set of writing skills. Scientific writing must communication scientific information in a concise and precise manner. Flowery, ambiguous, verbose, and redundant language have no place in this writing style. The ultimate goal of a PhD is to write a dissertation that is typically hundreds of pages long. According to this blog post, the average length of a computer science doctoral dissertation from a sample gathered at the University of Minnesota database is 103 pages. The hope is that this weekly blogging project will make the seemingly daunting task more achieveable.</p> <p>At the moment, I am not too sure what kind of content I will be writing on. I will most likely choose to write on what I learned during the course of the week. These may consist of organized notes on lecture material or particular topics that I have found interesting and have taken a further look at. The makeup of my blog posts will change as I continue with my program. I can already sense that I will be writing my own takes on research papers I have read to gain a better understanding of the subject matter.</p> <p>Hopefully at the end of my PhD, I can look back at all the blog posts I have written and feel accomplished with not only the improvement of quality of writing but just the sheer amount I will have written over the years.</p>]]></content><author><name></name></author><category term="misc"/><category term="misc"/><summary type="html"><![CDATA[Today marks the third week since the start of my PhD program at Washington University at St. Louis. Inspired by fellow PhD student Saumik, I’ve decided to commit to writing a weekly blog to track the progress of my maturation as a PhD student. You can check out Saumik’s website here.]]></summary></entry><entry><title type="html">The Face Behind the Handle</title><link href="https://jungwooseok.com/blog/2019/the-face-behind-the-handle-medium/" rel="alternate" type="text/html" title="The Face Behind the Handle"/><published>2019-12-03T00:00:00+00:00</published><updated>2019-12-03T00:00:00+00:00</updated><id>https://jungwooseok.com/blog/2019/the-face-behind-the-handle-medium</id><content type="html" xml:base="https://jungwooseok.com/blog/2019/the-face-behind-the-handle-medium/"><![CDATA[]]></content><author><name></name></author><category term="medium posts"/><category term="ML"/><category term="NLP"/><summary type="html"><![CDATA[Using neural networks to distinguish Donald Trump’s tweeting habits]]></summary></entry><entry><title type="html">Un-bottling the data</title><link href="https://jungwooseok.com/blog/2019/unbottling-the-data-medium/" rel="alternate" type="text/html" title="Un-bottling the data"/><published>2019-12-02T00:00:00+00:00</published><updated>2019-12-02T00:00:00+00:00</updated><id>https://jungwooseok.com/blog/2019/unbottling-the-data-medium</id><content type="html" xml:base="https://jungwooseok.com/blog/2019/unbottling-the-data-medium/"><![CDATA[]]></content><author><name></name></author><category term="medium posts"/><category term="ML"/><category term="NLP"/><summary type="html"><![CDATA[Exploring the big data world of wine]]></summary></entry></feed>